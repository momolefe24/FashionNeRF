{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e5c321-2657-4a81-a94c-b3cb7661d6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f112f1f7-d02c-4eca-876d-a1a433c0214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structures and functions for rendering\n",
    "from pytorch3d.structures import Volumes\n",
    "from pytorch3d.transforms import so3_exp_map\n",
    "from pytorch3d.renderer import (\n",
    "    FoVPerspectiveCameras, \n",
    "    NDCMultinomialRaysampler,\n",
    "    MonteCarloRaysampler,\n",
    "    EmissionAbsorptionRaymarcher,\n",
    "    ImplicitRenderer,\n",
    "    RayBundle,\n",
    "    ray_bundle_to_ray_points,\n",
    ")\n",
    "\n",
    "# obtain the utilized device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    print(\n",
    "        'Please note that NeRF is a resource-demanding method.'\n",
    "        + ' Running this notebook on CPU will be extremely slow.'\n",
    "        + ' We recommend running the example on a GPU'\n",
    "        + ' with at least 10 GB of memory.'\n",
    "    )\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4e5c08-ae73-43f5-a7a7-962501693ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37634ca5-a6f6-43e9-b461-d865cf940955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_image_grid import image_grid\n",
    "from generate_cow_renders import generate_cow_renders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006d3d82-023b-441e-8311-504f39f50fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 40 images/silhouettes/cameras.\n"
     ]
    }
   ],
   "source": [
    "target_cameras, target_images, target_silhouettes = generate_cow_renders(num_views=40, azimuth_range=180)\n",
    "print(f'Generated {len(target_images)} images/silhouettes/cameras.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc8bc16-114b-458e-9139-a390f30fa9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dca5fb-02fe-4e56-82b6-06b089b5e79d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "render_size = target_images.shape[1] * 2\n",
    "volume_extent_world = 3.0\n",
    "raysampler_grid = NDCMultinomialRaysampler(\n",
    "    image_height=render_size,\n",
    "    image_width=render_size,\n",
    "    n_pts_per_ray=128,\n",
    "    min_depth=0.1,\n",
    "    max_depth=volume_extent_world,\n",
    ")\n",
    "raysampler_mc = MonteCarloRaysampler(\n",
    "    min_x = -1.0,\n",
    "    max_x = 1.0,\n",
    "    min_y = -1.0,\n",
    "    max_y = 1.0,\n",
    "    n_rays_per_image=750,\n",
    "    n_pts_per_ray=128,\n",
    "    min_depth=0.1,\n",
    "    max_depth=volume_extent_world,\n",
    ")\n",
    "raymarcher = EmissionAbsorptionRaymarcher()\n",
    "renderer_grid = ImplicitRenderer(\n",
    "    raysampler=raysampler_grid, raymarcher=raymarcher,\n",
    ")\n",
    "renderer_mc = ImplicitRenderer(\n",
    "    raysampler=raysampler_mc, raymarcher=raymarcher,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f06efe2b-1f56-46c3-9f1c-4fb372b2749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HarmonicEmbedding(torch.nn.Module):\n",
    "    def __init__(self, n_harmonic_functions=60, omega0=0.1):\n",
    "        \"\"\"\n",
    "        Given an input tensor `x` of shape [minibatch, ... , dim],\n",
    "        the harmonic embedding layer converts each feature\n",
    "        in `x` into a series of harmonic features `embedding`\n",
    "        as follows:\n",
    "            embedding[..., i*dim:(i+1)*dim] = [\n",
    "                sin(x[..., i]),\n",
    "                sin(2*x[..., i]),\n",
    "                sin(4*x[..., i]),\n",
    "                ...\n",
    "                sin(2**(self.n_harmonic_functions-1) * x[..., i]),\n",
    "                cos(x[..., i]),\n",
    "                cos(2*x[..., i]),\n",
    "                cos(4*x[..., i]),\n",
    "                ...\n",
    "                cos(2**(self.n_harmonic_functions-1) * x[..., i])\n",
    "            ]\n",
    "            \n",
    "        Note that `x` is also premultiplied by `omega0` before\n",
    "        evaluating the harmonic functions.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.register_buffer(\n",
    "            'frequencies',\n",
    "            omega0 * (2.0 ** torch.arange(n_harmonic_functions)),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: tensor of shape [..., dim]\n",
    "        Returns:\n",
    "            embedding: a harmonic embedding of `x`\n",
    "                of shape [..., n_harmonic_functions * dim * 2]\n",
    "        \"\"\"\n",
    "        embed = (x[..., None] * self.frequencies).view(*x.shape[:-1], -1)\n",
    "        return torch.cat((embed.sin(), embed.cos()), dim=-1)\n",
    "\n",
    "\n",
    "class NeuralRadianceField(torch.nn.Module):\n",
    "    def __init__(self, n_harmonic_functions=60, n_hidden_neurons=256):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_harmonic_functions: The number of harmonic functions\n",
    "                used to form the harmonic embedding of each point.\n",
    "            n_hidden_neurons: The number of hidden units in the\n",
    "                fully connected layers of the MLPs of the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        # The harmonic embedding layer converts input 3D coordinates\n",
    "        # to a representation that is more suitable for\n",
    "        # processing with a deep neural network.\n",
    "        self.harmonic_embedding = HarmonicEmbedding(n_harmonic_functions)\n",
    "        \n",
    "        # The dimension of the harmonic embedding.\n",
    "        embedding_dim = n_harmonic_functions * 2 * 3\n",
    "        \n",
    "        # self.mlp is a simple 2-layer multi-layer perceptron\n",
    "        # which converts the input per-point harmonic embeddings\n",
    "        # to a latent representation.\n",
    "        # Not that we use Softplus activations instead of ReLU.\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embedding_dim, n_hidden_neurons),\n",
    "            torch.nn.Softplus(beta=10.0),\n",
    "            torch.nn.Linear(n_hidden_neurons, n_hidden_neurons),\n",
    "            torch.nn.Softplus(beta=10.0),\n",
    "        )        \n",
    "        \n",
    "        # Given features predicted by self.mlp, self.color_layer\n",
    "        # is responsible for predicting a 3-D per-point vector\n",
    "        # that represents the RGB color of the point.\n",
    "        self.color_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_hidden_neurons + embedding_dim, n_hidden_neurons),\n",
    "            torch.nn.Softplus(beta=10.0),\n",
    "            torch.nn.Linear(n_hidden_neurons, 3),\n",
    "            torch.nn.Sigmoid(),\n",
    "            # To ensure that the colors correctly range between [0-1],\n",
    "            # the layer is terminated with a sigmoid layer.\n",
    "        )  \n",
    "        \n",
    "        # The density layer converts the features of self.mlp\n",
    "        # to a 1D density value representing the raw opacity\n",
    "        # of each point.\n",
    "        self.density_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_hidden_neurons, 1),\n",
    "            torch.nn.Softplus(beta=10.0),\n",
    "            # Sofplus activation ensures that the raw opacity\n",
    "            # is a non-negative number.\n",
    "        )\n",
    "        \n",
    "        # We set the bias of the density layer to -1.5\n",
    "        # in order to initialize the opacities of the\n",
    "        # ray points to values close to 0. \n",
    "        # This is a crucial detail for ensuring convergence\n",
    "        # of the model.\n",
    "        self.density_layer[0].bias.data[0] = -1.5        \n",
    "                \n",
    "    def _get_densities(self, features):\n",
    "        \"\"\"\n",
    "        This function takes `features` predicted by `self.mlp`\n",
    "        and converts them to `raw_densities` with `self.density_layer`.\n",
    "        `raw_densities` are later mapped to [0-1] range with\n",
    "        1 - inverse exponential of `raw_densities`.\n",
    "        \"\"\"\n",
    "        raw_densities = self.density_layer(features)\n",
    "        return 1 - (-raw_densities).exp()\n",
    "    \n",
    "    def _get_colors(self, features, rays_directions):\n",
    "        \"\"\"\n",
    "        This function takes per-point `features` predicted by `self.mlp`\n",
    "        and evaluates the color model in order to attach to each\n",
    "        point a 3D vector of its RGB color.\n",
    "        \n",
    "        In order to represent viewpoint dependent effects,\n",
    "        before evaluating `self.color_layer`, `NeuralRadianceField`\n",
    "        concatenates to the `features` a harmonic embedding\n",
    "        of `ray_directions`, which are per-point directions \n",
    "        of point rays expressed as 3D l2-normalized vectors\n",
    "        in world coordinates.\n",
    "        \"\"\"\n",
    "        spatial_size = features.shape[:-1]\n",
    "        \n",
    "        # Normalize the ray_directions to unit l2 norm.\n",
    "        rays_directions_normed = torch.nn.functional.normalize(\n",
    "            rays_directions, dim=-1\n",
    "        )\n",
    "        \n",
    "        # Obtain the harmonic embedding of the normalized ray directions.\n",
    "        rays_embedding = self.harmonic_embedding(\n",
    "            rays_directions_normed\n",
    "        )\n",
    "        \n",
    "        # Expand the ray directions tensor so that its spatial size\n",
    "        # is equal to the size of features.\n",
    "        rays_embedding_expand = rays_embedding[..., None, :].expand(\n",
    "            *spatial_size, rays_embedding.shape[-1]\n",
    "        )\n",
    "        \n",
    "        # Concatenate ray direction embeddings with \n",
    "        # features and evaluate the color model.\n",
    "        color_layer_input = torch.cat(\n",
    "            (features, rays_embedding_expand),\n",
    "            dim=-1\n",
    "        )\n",
    "        return self.color_layer(color_layer_input)\n",
    "    \n",
    "  \n",
    "    def forward(\n",
    "        self, \n",
    "        ray_bundle: RayBundle,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The forward function accepts the parametrizations of\n",
    "        3D points sampled along projection rays. The forward\n",
    "        pass is responsible for attaching a 3D vector\n",
    "        and a 1D scalar representing the point's \n",
    "        RGB color and opacity respectively.\n",
    "        \n",
    "        Args:\n",
    "            ray_bundle: A RayBundle object containing the following variables:\n",
    "                origins: A tensor of shape `(minibatch, ..., 3)` denoting the\n",
    "                    origins of the sampling rays in world coords.\n",
    "                directions: A tensor of shape `(minibatch, ..., 3)`\n",
    "                    containing the direction vectors of sampling rays in world coords.\n",
    "                lengths: A tensor of shape `(minibatch, ..., num_points_per_ray)`\n",
    "                    containing the lengths at which the rays are sampled.\n",
    "\n",
    "        Returns:\n",
    "            rays_densities: A tensor of shape `(minibatch, ..., num_points_per_ray, 1)`\n",
    "                denoting the opacity of each ray point.\n",
    "            rays_colors: A tensor of shape `(minibatch, ..., num_points_per_ray, 3)`\n",
    "                denoting the color of each ray point.\n",
    "        \"\"\"\n",
    "        # We first convert the ray parametrizations to world\n",
    "        # coordinates with `ray_bundle_to_ray_points`.\n",
    "        rays_points_world = ray_bundle_to_ray_points(ray_bundle)\n",
    "        # rays_points_world.shape = [minibatch x ... x 3]\n",
    "        \n",
    "        # For each 3D world coordinate, we obtain its harmonic embedding.\n",
    "        embeds = self.harmonic_embedding(\n",
    "            rays_points_world\n",
    "        )\n",
    "        # embeds.shape = [minibatch x ... x self.n_harmonic_functions*6]\n",
    "        \n",
    "        # self.mlp maps each harmonic embedding to a latent feature space.\n",
    "        features = self.mlp(embeds)\n",
    "        # features.shape = [minibatch x ... x n_hidden_neurons]\n",
    "        \n",
    "        # Finally, given the per-point features, \n",
    "        # execute the density and color branches.\n",
    "        \n",
    "        rays_densities = self._get_densities(features)\n",
    "        # rays_densities.shape = [minibatch x ... x 1]\n",
    "\n",
    "        rays_colors = self._get_colors(features, ray_bundle.directions)\n",
    "        # rays_colors.shape = [minibatch x ... x 3]\n",
    "        \n",
    "        return rays_densities, rays_colors\n",
    "    \n",
    "    def batched_forward(\n",
    "        self, \n",
    "        ray_bundle: RayBundle,\n",
    "        n_batches: int = 16,\n",
    "        **kwargs,        \n",
    "    ):\n",
    "        \"\"\"\n",
    "        This function is used to allow for memory efficient processing\n",
    "        of input rays. The input rays are first split to `n_batches`\n",
    "        chunks and passed through the `self.forward` function one at a time\n",
    "        in a for loop. Combined with disabling PyTorch gradient caching\n",
    "        (`torch.no_grad()`), this allows for rendering large batches\n",
    "        of rays that do not all fit into GPU memory in a single forward pass.\n",
    "        In our case, batched_forward is used to export a fully-sized render\n",
    "        of the radiance field for visualization purposes.\n",
    "        \n",
    "        Args:\n",
    "            ray_bundle: A RayBundle object containing the following variables:\n",
    "                origins: A tensor of shape `(minibatch, ..., 3)` denoting the\n",
    "                    origins of the sampling rays in world coords.\n",
    "                directions: A tensor of shape `(minibatch, ..., 3)`\n",
    "                    containing the direction vectors of sampling rays in world coords.\n",
    "                lengths: A tensor of shape `(minibatch, ..., num_points_per_ray)`\n",
    "                    containing the lengths at which the rays are sampled.\n",
    "            n_batches: Specifies the number of batches the input rays are split into.\n",
    "                The larger the number of batches, the smaller the memory footprint\n",
    "                and the lower the processing speed.\n",
    "\n",
    "        Returns:\n",
    "            rays_densities: A tensor of shape `(minibatch, ..., num_points_per_ray, 1)`\n",
    "                denoting the opacity of each ray point.\n",
    "            rays_colors: A tensor of shape `(minibatch, ..., num_points_per_ray, 3)`\n",
    "                denoting the color of each ray point.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Parse out shapes needed for tensor reshaping in this function.\n",
    "        n_pts_per_ray = ray_bundle.lengths.shape[-1]  \n",
    "        spatial_size = [*ray_bundle.origins.shape[:-1], n_pts_per_ray]\n",
    "\n",
    "        # Split the rays to `n_batches` batches.\n",
    "        tot_samples = ray_bundle.origins.shape[:-1].numel()\n",
    "        batches = torch.chunk(torch.arange(tot_samples), n_batches)\n",
    "\n",
    "        # For each batch, execute the standard forward pass.\n",
    "        batch_outputs = [\n",
    "            self.forward(\n",
    "                RayBundle(\n",
    "                    origins=ray_bundle.origins.view(-1, 3)[batch_idx],\n",
    "                    directions=ray_bundle.directions.view(-1, 3)[batch_idx],\n",
    "                    lengths=ray_bundle.lengths.view(-1, n_pts_per_ray)[batch_idx],\n",
    "                    xys=None,\n",
    "                )\n",
    "            ) for batch_idx in batches\n",
    "        ]\n",
    "        \n",
    "        # Concatenate the per-batch rays_densities and rays_colors\n",
    "        # and reshape according to the sizes of the inputs.\n",
    "        rays_densities, rays_colors = [\n",
    "            torch.cat(\n",
    "                [batch_output[output_i] for batch_output in batch_outputs], dim=0\n",
    "            ).view(*spatial_size, -1) for output_i in (0, 1)\n",
    "        ]\n",
    "        return rays_densities, rays_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5eeb90bf-ec9d-40e1-ad00-0578daf1c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber(x, y, scaling=0.1):\n",
    "    diff_sq = (x - y) ** 2\n",
    "    loss = ((1 + diff_sq / (scaling**2)).clamp(1e-4).sqrt() - 1) * float(scaling)\n",
    "    return loss\n",
    "\n",
    "def sample_images_at_mc_locs(target_images, sampled_rays_xy):\n",
    "    ba = target_images.shape[0]\n",
    "    dim = target_images.shape[-1]\n",
    "    spatial_size = sampled_rays_xy.shape[1:-1]\n",
    "    images_sampled = torch.nn.functional.grid_sample(\n",
    "        target_images.permute(0, 3, 1, 2), \n",
    "        -sampled_rays_xy.view(ba, -1, 1, 2),  # note the sign inversion\n",
    "        align_corners=True\n",
    "    )\n",
    "    return images_sampled.permute(0, 2, 3, 1).view(\n",
    "        ba, *spatial_size, dim\n",
    "    )\n",
    "\n",
    "def show_full_render(\n",
    "    neural_radiance_field, camera,\n",
    "    target_image, target_silhouette,\n",
    "    loss_history_color, loss_history_sil,\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        rendered_image_silhouette, _ = renderer_grid(\n",
    "            cameras=camera, \n",
    "            volumetric_function=neural_radiance_field.batched_forward\n",
    "        )\n",
    "        rendered_image, rendered_silhouette = (\n",
    "            rendered_image_silhouette[0].split([3, 1], dim=-1)\n",
    "        )\n",
    "        \n",
    "    # Generate plots.\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    ax = ax.ravel()\n",
    "    clamp_and_detach = lambda x: x.clamp(0.0, 1.0).cpu().detach().numpy()\n",
    "    ax[0].plot(list(range(len(loss_history_color))), loss_history_color, linewidth=1)\n",
    "    ax[1].imshow(clamp_and_detach(rendered_image))\n",
    "    ax[2].imshow(clamp_and_detach(rendered_silhouette[..., 0]))\n",
    "    ax[3].plot(list(range(len(loss_history_sil))), loss_history_sil, linewidth=1)\n",
    "    ax[4].imshow(clamp_and_detach(target_image))\n",
    "    ax[5].imshow(clamp_and_detach(target_silhouette))\n",
    "    for ax_, title_ in zip(\n",
    "        ax,\n",
    "        (\n",
    "            \"loss color\", \"rendered image\", \"rendered silhouette\",\n",
    "            \"loss silhouette\", \"target image\",  \"target silhouette\",\n",
    "        )\n",
    "    ):\n",
    "        if not title_.startswith('loss'):\n",
    "            ax_.grid(\"off\")\n",
    "            ax_.axis(\"off\")\n",
    "        ax_.set_title(title_)\n",
    "    fig.canvas.draw(); fig.show()\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0805c5e9-425c-4a9a-a4c9-96d4c01da158",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 564.00 MiB (GPU 0; 6.00 GiB total capacity; 4.88 GiB already allocated; 0 bytes free; 4.97 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m batch_cameras \u001b[38;5;241m=\u001b[39m FoVPerspectiveCameras(\n\u001b[1;32m     48\u001b[0m     R \u001b[38;5;241m=\u001b[39m target_cameras\u001b[38;5;241m.\u001b[39mR[batch_idx], \n\u001b[1;32m     49\u001b[0m     T \u001b[38;5;241m=\u001b[39m target_cameras\u001b[38;5;241m.\u001b[39mT[batch_idx], \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     device \u001b[38;5;241m=\u001b[39m device,\n\u001b[1;32m     55\u001b[0m )\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Evaluate the nerf model.\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m rendered_images_silhouettes, sampled_rays \u001b[38;5;241m=\u001b[39m \u001b[43mrenderer_mc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcameras\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_cameras\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvolumetric_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneural_radiance_field\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m rendered_images, rendered_silhouettes \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     63\u001b[0m     rendered_images_silhouettes\u001b[38;5;241m.\u001b[39msplit([\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Compute the silhouette error as the mean huber\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# loss between the predicted masks and the\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# sampled target silhouettes.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/pytorch3d/renderer/implicit/renderer.py:168\u001b[0m, in \u001b[0;36mImplicitRenderer.forward\u001b[0;34m(self, cameras, volumetric_function, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m ray_bundle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraysampler(\n\u001b[1;32m    158\u001b[0m     cameras\u001b[38;5;241m=\u001b[39mcameras, volumetric_function\u001b[38;5;241m=\u001b[39mvolumetric_function, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    159\u001b[0m )\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# ray_bundle.origins - minibatch x ... x 3\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# ray_bundle.directions - minibatch x ... x 3\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# ray_bundle.lengths - minibatch x ... x n_pts_per_ray\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# evaluates the densities and features at the locations of the\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# ray points\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m rays_densities, rays_features \u001b[38;5;241m=\u001b[39m \u001b[43mvolumetric_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mray_bundle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mray_bundle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcameras\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcameras\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# ray_densities - minibatch x ... x n_pts_per_ray x density_dim\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# ray_features - minibatch x ... x n_pts_per_ray x feature_dim\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# finally, march along the sampled rays to obtain the renders\u001b[39;00m\n\u001b[1;32m    175\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraymarcher(\n\u001b[1;32m    176\u001b[0m     rays_densities\u001b[38;5;241m=\u001b[39mrays_densities,\n\u001b[1;32m    177\u001b[0m     rays_features\u001b[38;5;241m=\u001b[39mrays_features,\n\u001b[1;32m    178\u001b[0m     ray_bundle\u001b[38;5;241m=\u001b[39mray_bundle,\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    180\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mNeuralRadianceField.forward\u001b[0;34m(self, ray_bundle, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m rays_densities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_densities(features)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# rays_densities.shape = [minibatch x ... x 1]\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m rays_colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_colors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mray_bundle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirections\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# rays_colors.shape = [minibatch x ... x 3]\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rays_densities, rays_colors\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mNeuralRadianceField._get_colors\u001b[0;34m(self, features, rays_directions)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# Concatenate ray direction embeddings with \u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# features and evaluate the color model.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m color_layer_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m    144\u001b[0m     (features, rays_embedding_expand),\n\u001b[1;32m    145\u001b[0m     dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    146\u001b[0m )\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolor_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolor_layer_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/nn/modules/activation.py:826\u001b[0m, in \u001b[0;36mSoftplus.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftplus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 564.00 MiB (GPU 0; 6.00 GiB total capacity; 4.88 GiB already allocated; 0 bytes free; 4.97 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# First move all relevant variables to the correct device.\n",
    "renderer_grid = renderer_grid.to(device)\n",
    "renderer_mc = renderer_mc.to(device)\n",
    "target_cameras = target_cameras.to(device)\n",
    "target_images = target_images.to(device)\n",
    "target_silhouettes = target_silhouettes.to(device)\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Instantiate the radiance field model.\n",
    "neural_radiance_field = NeuralRadianceField().to(device)\n",
    "\n",
    "# Instantiate the Adam optimizer. We set its master learning rate to 1e-3.\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(neural_radiance_field.parameters(), lr=lr)\n",
    "\n",
    "# We sample 6 random cameras in a minibatch. Each camera\n",
    "# emits raysampler_mc.n_pts_per_image rays.\n",
    "batch_size = 3\n",
    "\n",
    "# 3000 iterations take ~20 min on a Tesla M40 and lead to\n",
    "# reasonably sharp results. However, for the best possible\n",
    "# results, we recommend setting n_iter=20000.\n",
    "n_iter = 3000\n",
    "\n",
    "# Init the loss history buffers.\n",
    "loss_history_color, loss_history_sil = [], []\n",
    "\n",
    "# The main optimization loop.\n",
    "for iteration in range(n_iter):      \n",
    "    # In case we reached the last 75% of iterations,\n",
    "    # decrease the learning rate of the optimizer 10-fold.\n",
    "    if iteration == round(n_iter * 0.75):\n",
    "        print('Decreasing LR 10-fold ...')\n",
    "        optimizer = torch.optim.Adam(\n",
    "            neural_radiance_field.parameters(), lr=lr * 0.1\n",
    "        )\n",
    "    \n",
    "    # Zero the optimizer gradient.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Sample random batch indices.\n",
    "    batch_idx = torch.randperm(len(target_cameras))[:batch_size]\n",
    "    \n",
    "    # Sample the minibatch of cameras.\n",
    "    batch_cameras = FoVPerspectiveCameras(\n",
    "        R = target_cameras.R[batch_idx], \n",
    "        T = target_cameras.T[batch_idx], \n",
    "        znear = target_cameras.znear[batch_idx],\n",
    "        zfar = target_cameras.zfar[batch_idx],\n",
    "        aspect_ratio = target_cameras.aspect_ratio[batch_idx],\n",
    "        fov = target_cameras.fov[batch_idx],\n",
    "        device = device,\n",
    "    )\n",
    "    \n",
    "    # Evaluate the nerf model.\n",
    "    rendered_images_silhouettes, sampled_rays = renderer_mc(\n",
    "        cameras=batch_cameras, \n",
    "        volumetric_function=neural_radiance_field\n",
    "    )\n",
    "    rendered_images, rendered_silhouettes = (\n",
    "        rendered_images_silhouettes.split([3, 1], dim=-1)\n",
    "    )\n",
    "    \n",
    "    # Compute the silhouette error as the mean huber\n",
    "    # loss between the predicted masks and the\n",
    "    # sampled target silhouettes.\n",
    "    silhouettes_at_rays = sample_images_at_mc_locs(\n",
    "        target_silhouettes[batch_idx, ..., None], \n",
    "        sampled_rays.xys\n",
    "    )\n",
    "    sil_err = huber(\n",
    "        rendered_silhouettes, \n",
    "        silhouettes_at_rays,\n",
    "    ).abs().mean()\n",
    "\n",
    "    # Compute the color error as the mean huber\n",
    "    # loss between the rendered colors and the\n",
    "    # sampled target images.\n",
    "    colors_at_rays = sample_images_at_mc_locs(\n",
    "        target_images[batch_idx], \n",
    "        sampled_rays.xys\n",
    "    )\n",
    "    color_err = huber(\n",
    "        rendered_images, \n",
    "        colors_at_rays,\n",
    "    ).abs().mean()\n",
    "    \n",
    "    # The optimization loss is a simple\n",
    "    # sum of the color and silhouette errors.\n",
    "    loss = color_err + sil_err\n",
    "    \n",
    "    # Log the loss history.\n",
    "    loss_history_color.append(float(color_err))\n",
    "    loss_history_sil.append(float(sil_err))\n",
    "    \n",
    "    # Every 10 iterations, print the current values of the losses.\n",
    "    if iteration % 10 == 0:\n",
    "        print(\n",
    "            f'Iteration {iteration:05d}:'\n",
    "            + f' loss color = {float(color_err):1.2e}'\n",
    "            + f' loss silhouette = {float(sil_err):1.2e}'\n",
    "        )\n",
    "    \n",
    "    # Take the optimization step.\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Visualize the full renders every 100 iterations.\n",
    "    if iteration % 100 == 0:\n",
    "        show_idx = torch.randperm(len(target_cameras))[:1]\n",
    "        show_full_render(\n",
    "            neural_radiance_field,\n",
    "            FoVPerspectiveCameras(\n",
    "                R = target_cameras.R[show_idx], \n",
    "                T = target_cameras.T[show_idx], \n",
    "                znear = target_cameras.znear[show_idx],\n",
    "                zfar = target_cameras.zfar[show_idx],\n",
    "                aspect_ratio = target_cameras.aspect_ratio[show_idx],\n",
    "                fov = target_cameras.fov[show_idx],\n",
    "                device = device,\n",
    "            ), \n",
    "            target_images[show_idx][0],\n",
    "            target_silhouettes[show_idx][0],\n",
    "            loss_history_color,\n",
    "            loss_history_sil,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6525c6-b143-4956-99e9-786dd0b8226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rotating_nerf(neural_radiance_field, n_frames = 50):\n",
    "    logRs = torch.zeros(n_frames, 3, device=device)\n",
    "    logRs[:, 1] = torch.linspace(-3.14, 3.14, n_frames, device=device)\n",
    "    Rs = so3_exp_map(logRs)\n",
    "    Ts = torch.zeros(n_frames, 3, device=device)\n",
    "    Ts[:, 2] = 2.7\n",
    "    frames = []\n",
    "    print('Rendering rotating NeRF ...')\n",
    "    for R, T in zip(tqdm(Rs), Ts):\n",
    "        camera = FoVPerspectiveCameras(\n",
    "            R=R[None], \n",
    "            T=T[None], \n",
    "            znear=target_cameras.znear[0],\n",
    "            zfar=target_cameras.zfar[0],\n",
    "            aspect_ratio=target_cameras.aspect_ratio[0],\n",
    "            fov=target_cameras.fov[0],\n",
    "            device=device,\n",
    "        )\n",
    "        # Note that we again render with `NDCMultinomialRaysampler`\n",
    "        # and the batched_forward function of neural_radiance_field.\n",
    "        frames.append(\n",
    "            renderer_grid(\n",
    "                cameras=camera, \n",
    "                volumetric_function=neural_radiance_field.batched_forward,\n",
    "            )[0][..., :3]\n",
    "        )\n",
    "    return torch.cat(frames)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    rotating_nerf_frames = generate_rotating_nerf(neural_radiance_field, n_frames=3*5)\n",
    "    \n",
    "image_grid(rotating_nerf_frames.clamp(0., 1.).cpu().numpy(), rows=3, cols=5, rgb=True, fill=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7fb6ae-96b6-4b7a-947a-43f72c0d6e12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15734cb-7920-488c-b758-b82b008f8278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "pytorch3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
